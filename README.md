Cyber Bully Detection System

DistilBERT + Logistic Regression Fallback

ğŸ” Overview

This project is a real-time cyberbullying detection web application built using:

Fine-tuned DistilBERT (primary model)

TF-IDF + Logistic Regression (fallback model)

Flask web application

SQLite for flagged message logging

The system classifies text into:

hate_speech

offensive

safe

It combines deep learning with rule-based escalation to reduce false negatives.

ğŸ§  Model Architecture
1ï¸âƒ£ Primary Model

Fine-tuned DistilBERT (Hugging Face Transformers)

Tokenized with DistilBERT tokenizer

Softmax classification head

Confidence-based thresholding

Runs on CPU/GPU

2ï¸âƒ£ Fallback Model

TF-IDF + Logistic Regression (scikit-learn)

Used if:

DistilBERT fails to load

Or inference errors occur

3ï¸âƒ£ Advisory Layer (High Precision Rules)

Certain patterns are immediately escalated:

Explicit racial slurs

Direct violent threats

Explicit hate speech indicators

This prevents low-confidence misses.

ğŸ–¥ Web Application

Built with:

Flask

Gunicorn (for production)

SQLite database

Features:

Real-time chat-style interface

Confidence score display

Admin panel for flagged messages

Statistics dashboard

REST API endpoint (/api/analyze)

ğŸ“ Project Structure
cyber_pj/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ templates/
â”œâ”€â”€ static/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ distilbert/
â”‚   â””â”€â”€ model_fallback.joblib
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

âš™ï¸ Installation (Local Setup)
1ï¸âƒ£ Create virtual environment
python -m venv venv
venv\Scripts\activate

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run application
python src/app.py


Open:

http://127.0.0.1:7860

ğŸ³ Docker Deployment (Optional)

Build image:

docker build -t cyber-bully-detector .


Run container:

docker run -p 7860:7860 cyber-bully-detector

ğŸŒ Hugging Face Deployment

Live Demo:
ğŸ‘‰ [YOUR_HUGGINGFACE_LINK_HERE]

ğŸ“Š Example Predictions
Input	Prediction
I love my family	SAFE
You idiot	OFFENSIVE
You filthy nigger	HATE_SPEECH
I will kill you	HATE_SPEECH
ğŸ›¡ Safety Design Philosophy

This system prioritizes:

Minimizing false negatives for hate speech

Conservative rule escalation

Confidence threshold control

Hybrid ML + rule-based detection

ğŸš€ Future Improvements

Better contextual bias detection

Multi-language support

Adversarial robustness testing

Model quantization for faster inference

Confidence calibration tuning

ğŸ“Œ Technologies Used

Python 3.10

PyTorch

Hugging Face Transformers

scikit-learn

Flask

SQLite

Gunicorn

ğŸ“„ License

This project is for educational and research purposes.